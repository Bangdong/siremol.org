{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytest\n",
    "\n",
    "[Pytest](https://docs.pytest.org/en/latest/contents.html) has rapidly established itself as the standard python testing framework. Its power lies in its simplicity, which makes it super easy to write tests and to run them. Tests can be as simple as the ones that we've already seen, so it is perfect for small projects. However, hidden behind the simple exterior, `pytest` provides a great deal of power and flexibility, meaning that it scales well to large and complex projects.\n",
    "\n",
    "In this section we will learn how to run tests using `pytest` and how to take advantage of the powerful `pytest.mark` helper in order to add useful attributes to our tests.\n",
    "\n",
    "## Running tests\n",
    "\n",
    "Pytest comes with a command-line tool called, unsurprisingly, `pytest`. (With older versions of python this was called `py.test`.) When run, `pytest` searches for test files in all directories and files below the current directory, collects the tests together, then runs them. Pytest uses name matching to locate the test files. Valid names start or end with `test`, e.g.\n",
    "\n",
    "```bash\n",
    "test_example.py    # We will use this as our standard naming convention.\n",
    "example_test.py\n",
    "```\n",
    "\n",
    "(Note that this naming convention applies to test functions, as well as files.)\n",
    "\n",
    "You can specify one or more paths and `pytest` will only look for test files in those paths, e.g.\n",
    "\n",
    "```bash\n",
    "pytest /path/to/my/awesome/module\n",
    "```\n",
    "\n",
    "When writing a python module it is good practice to set up a directory structure in order to keep things tidy. Throughout this course we will use the following:\n",
    "\n",
    "```bash\n",
    "mypkg/\n",
    "    __init__.py\n",
    "    whizz.py\n",
    "    bang.py\n",
    "    test/\n",
    "        __init__.py\n",
    "        test_whizz.py\n",
    "        test_bang.py\n",
    "```\n",
    "\n",
    "Here the `__init__.py` files makes python aware that the directories should be treated as modules. Assuming we're in the top level directory (where out notebooks reside) this allows us to do the following:\n",
    "\n",
    "```python\n",
    "import mypkg\n",
    "```\n",
    "\n",
    "Let's dive in and run some tests using `mymodule` that was introduced in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.3, pytest-3.3.0, py-1.5.0, pluggy-0.6.0\n",
      "rootdir: /home/lester/Code/siremol.org/chryswoods.com/python_and_data/testing, inifile:\n",
      "collected 16 items                                                             \u001b[0m\u001b[1m\n",
      "\n",
      "mymodule/test/test_errors.py .s\u001b[36m                                          [ 12%]\u001b[0m\n",
      "mymodule/test/test_mymath.py F.......s..Fx.\u001b[36m                              [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m___________________________________ test_add ___________________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_add():\u001b[0m\n",
      "\u001b[1m        \"\"\" Test the add function. \"\"\"\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        assert add(1, 1) ==  2\u001b[0m\n",
      "\u001b[1m>       assert add(1, 2) == add(2, 1) == 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2 == 4\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2 = add(1, 2)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  and   4 = add(2, 1)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mmymodule/test/test_mymath.py\u001b[0m:11: AssertionError\n",
      "\u001b[31m\u001b[1m____________________________ test_greaterThan[3-7] _____________________________\u001b[0m\n",
      "[XPASS(strict)] \n",
      "\u001b[31m\u001b[1m========== 2 failed, 11 passed, 2 skipped, 1 xfailed in 8.62 seconds ===========\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest mymodule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What just happened?\n",
    "\n",
    "Well, `pytest` searched within the `mymodule` directory and collected a total of 16 tests. These tests were spread accross two files:\n",
    "\n",
    "```bash\n",
    "mymodule/test/test_errors.py\n",
    "mymodule/test/test_mymath.py\n",
    "```\n",
    "\n",
    "The tests were then run with a single failure reported for the test `test_add`. This was a test for the `add` function, which we found to be broken in the previous section.\n",
    "\n",
    "What are all the cryptic symbols next to the name of the each files?\n",
    "\n",
    "```bash\n",
    "F.......s..Xx\n",
    "```\n",
    "\n",
    "To get more detailed information about each test, we can run `pytest` in _verbose_ mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.3, pytest-3.3.0, py-1.5.0, pluggy-0.6.0 -- /usr/bin/python\n",
      "cachedir: .cache\n",
      "rootdir: /home/lester/Code/siremol.org/chryswoods.com/python_and_data/testing, inifile:\n",
      "collected 16 items                                                             \u001b[0m\u001b[1m\n",
      "\n",
      "mymodule/test/test_errors.py::test_indexError \u001b[32mPASSED\u001b[0m\u001b[36m                     [  6%]\u001b[0m\n",
      "mymodule/test/test_errors.py::test_BSoD \u001b[33mSKIPPED\u001b[0m\u001b[36m                          [ 12%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_add \u001b[31mFAILED\u001b[0m\u001b[36m                            [ 18%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_sub[1-2--1] \u001b[32mPASSED\u001b[0m\u001b[36m                    [ 25%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_sub[7-3-4] \u001b[32mPASSED\u001b[0m\u001b[36m                     [ 31%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_sub[21-58--37] \u001b[32mPASSED\u001b[0m\u001b[36m                 [ 37%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_mul[1-3] \u001b[32mPASSED\u001b[0m\u001b[36m                       [ 43%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_mul[1-4] \u001b[32mPASSED\u001b[0m\u001b[36m                       [ 50%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_mul[2-3] \u001b[32mPASSED\u001b[0m\u001b[36m                       [ 56%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_mul[2-4] \u001b[32mPASSED\u001b[0m\u001b[36m                       [ 62%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_div \u001b[33mSKIPPED\u001b[0m\u001b[36m                           [ 68%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_greaterThan[2-1] \u001b[32mPASSED\u001b[0m\u001b[36m               [ 75%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_greaterThan[9-4] \u001b[32mPASSED\u001b[0m\u001b[36m               [ 81%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_greaterThan[3-7] \u001b[33mXPASS\u001b[0m\u001b[36m                [ 87%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_greaterThan[6-11] \u001b[33mxfail\u001b[0m\u001b[36m               [ 93%]\u001b[0m\n",
      "mymodule/test/test_mymath.py::test_bigSum \u001b[32mPASSED\u001b[0m\u001b[36m                         [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m___________________________________ test_add ___________________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_add():\u001b[0m\n",
      "\u001b[1m        \"\"\" Test the add function. \"\"\"\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        assert add(1, 1) ==  2\u001b[0m\n",
      "\u001b[1m>       assert add(1, 2) == add(2, 1) == 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2 == 4\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2 = add(1, 2)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  and   4 = add(2, 1)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mmymodule/test/test_mymath.py\u001b[0m:11: AssertionError\n",
      "\u001b[31m\u001b[1m===== 1 failed, 11 passed, 2 skipped, 1 xfailed, 1 xpassed in 8.70 seconds =====\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest mymodule -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have more detailed information about each test. Matching up the output with the symbols we can see that...\n",
    "\n",
    "```\n",
    ". = PASSED\n",
    "s = SKIPPED\n",
    "F = FAILED\n",
    "x = xfail\n",
    "X = XPASS\n",
    "```\n",
    "\n",
    "What do `xfail` and `XPASS` mean, and why were some tests skipped? Also, note that some tests were run mutliple times, e.g. `test_sub`. What do the numbers in square brackets mean?\n",
    "\n",
    "To _report_ more information on tests that were `SKIPPED`, `XPASS`, or `xfail`, we can run `pytest` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.3, pytest-3.3.0, py-1.5.0, pluggy-0.6.0\n",
      "rootdir: /home/lester/Code/siremol.org/chryswoods.com/python_and_data/testing, inifile:\n",
      "collected 16 items                                                             \u001b[0m\u001b[1m\n",
      "\n",
      "mymodule/test/test_errors.py .s\u001b[36m                                          [ 12%]\u001b[0m\n",
      "mymodule/test/test_mymath.py F.......s..Xx.\u001b[36m                              [100%]\u001b[0m\n",
      "=========================== short test summary info ============================\n",
      "SKIP [1] mymodule/test/test_errors.py:12: Only runs on windows.\n",
      "SKIP [1] mymodule/test/test_mymath.py:29: Not yet implemented.\n",
      "XFAIL mymodule/test/test_mymath.py::test_greaterThan[6-11]\n",
      "XPASS mymodule/test/test_mymath.py::test_greaterThan[3-7] \n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m___________________________________ test_add ___________________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_add():\u001b[0m\n",
      "\u001b[1m        \"\"\" Test the add function. \"\"\"\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        assert add(1, 1) ==  2\u001b[0m\n",
      "\u001b[1m>       assert add(1, 2) == add(2, 1) == 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2 == 4\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2 = add(1, 2)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  and   4 = add(2, 1)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mmymodule/test/test_mymath.py\u001b[0m:11: AssertionError\n",
      "\u001b[31m\u001b[1m===== 1 failed, 11 passed, 2 skipped, 1 xfailed, 1 xpassed in 9.01 seconds =====\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest mymodule -rsxX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have addition information about these tests. One test was skipped because it is only valid on Windows, another because it was testing functionality that hasn't been implemented yet.\n",
    "\n",
    "By now, you might have guessed that `xfail` means _expected to fail_. You can see that one test, `test_greaterThan`, had and expected failure. Here the numbers in square brackets indicate the arguments to the test function. We are expecting that 6 isn't greater than 11, which is indeed correctly reported as an expected failure.\n",
    "\n",
    "But hang on, what about the `XPASS`? Is this an _expected pass_?\n",
    "\n",
    "No, `XPASS` means that an expected _failure_ actually _passed_! We were testing that 3 shouldn't be greater than 7, but somehow it passed. This means that there must be a bug in our code. (Note, however, that `test_greaterThan` didn't appear in the list of failures that were output by `pytest`. We'll cover this in the next section.)\n",
    "\n",
    "Phew, if all that was too much then you can always run `pytest` in _quiet_ mode. Here we only see a minimal output showing the progress of the tests and reports for any failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".sF.......s..Xx.\u001b[36m                                                         [100%]\u001b[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m___________________________________ test_add ___________________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_add():\u001b[0m\n",
      "\u001b[1m        \"\"\" Test the add function. \"\"\"\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        assert add(1, 1) ==  2\u001b[0m\n",
      "\u001b[1m>       assert add(1, 2) == add(2, 1) == 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2 == 4\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2 = add(1, 2)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  and   4 = add(2, 1)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mmymodule/test/test_mymath.py\u001b[0m:11: AssertionError\n",
      "\u001b[31m\u001b[1m1 failed, 11 passed, 2 skipped, 1 xfailed, 1 xpassed in 8.71 seconds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest mymodule -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "More details on how to invoke `pytest` can be found [here](https://docs.pytest.org/en/latest/usage.html).\n",
    "\n",
    "__Exercise 1:__ Make `pytest` stop after the first failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.6.3, pytest-3.3.0, py-1.5.0, pluggy-0.6.0\n",
      "rootdir: /home/lester/Code/siremol.org/chryswoods.com/python_and_data/testing, inifile:\n",
      "collected 16 items                                                             \u001b[0m\u001b[1m\n",
      "\n",
      "mymodule/test/test_errors.py .s\u001b[36m                                          [ 12%]\u001b[0m\n",
      "mymodule/test/test_mymath.py F\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m___________________________________ test_add ___________________________________\u001b[0m\n",
      "\n",
      "\u001b[1m    def test_add():\u001b[0m\n",
      "\u001b[1m        \"\"\" Test the add function. \"\"\"\u001b[0m\n",
      "\u001b[1m    \u001b[0m\n",
      "\u001b[1m        assert add(1, 1) ==  2\u001b[0m\n",
      "\u001b[1m>       assert add(1, 2) == add(2, 1) == 3\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 2 == 4\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 2 = add(1, 2)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  and   4 = add(2, 1)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mmymodule/test/test_mymath.py\u001b[0m:11: AssertionError\n",
      "\u001b[31m\u001b[1m================ 1 failed, 1 passed, 1 skipped in 0.04 seconds =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest mymodule -x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2:__ Only run tests for the `test_mul` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.6.3, pytest-3.3.0, py-1.5.0, pluggy-0.6.0\r\n",
      "rootdir: /home/lester/Code/siremol.org/chryswoods.com/python_and_data/testing, inifile:\r\n",
      "\u001b[1m\r",
      "collecting 4 items                                                             \u001b[0m\u001b[1m\r",
      "collected 4 items                                                              \u001b[0m\r\n",
      "\r\n",
      "mymodule/test/test_mymath.py ....\u001b[36m                                        [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m\u001b[1m=========================== 4 passed in 0.01 seconds ===========================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pytest mymodule/test/test_mymath.py::test_mul"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
